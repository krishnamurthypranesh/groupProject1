{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code for cleaning the rounds2 file of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Style\n",
    "    - Case: \n",
    "        - snake_case for objects\n",
    "        - camelCase for functions and classes\n",
    "    - Double quotes first, then single quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries used\n",
    "    - Pandas\n",
    "    - Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obejctives of Analysis\n",
    "Identify the most heavily invested main sectors in each of the three countries (for funding type FT and investments range of 5-15 M USD).\n",
    "\n",
    "Business objective: Identify the best: a. Sectors; b. Countries; c. Investment rounds for Spark Funds.\n",
    "\n",
    "[This means that we need to focus on just a few variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "Mean amount of money invested in a particular country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Workflow\n",
    "The workflow for this analysis is rather simple. Focus on answering the questions asked in the checkpoints. Following this flow, the code in this .ipynb is organized according to the checkpoints. There will be a clear heading indicating the starting and ending of each checkpoint and question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# importing dependencies\n",
    "# numpy\n",
    "import numpy as np # version: 1.15.0\n",
    "\n",
    "# pandas\n",
    "import pandas as pd # version: 0.23.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1: Data Cleaning\n",
    "There are five tasks in this checkpoint:\n",
    "    - Number of unique companies in rounds2.csv\n",
    "\t- Number of unique companies in companies.tsv\n",
    "\t- Key column from the companies dataset that can be used to merge it with rounds data\n",
    "\t- Organizations in companies that are missing in rounds2.\n",
    "    - Merge the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "The first step of the analysis is to import the two main datasets that we will be needing for the analysis: companies and rounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import companies.csv as companies\n",
    "companies = pd.read_csv(\"../../Data/companies.tsv\", sep = \"\\t\", encoding = \"ISO-8859-1\") \n",
    "\n",
    "# import rounds2.csv as rounds\n",
    "rounds = pd.read_csv(\"../../Data/rounds2.csv\", sep = \",\", encoding = \"ISO-8859-1\")\n",
    "# ISO for lack of charset in UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 66368 entries, 0 to 66367\n",
      "Data columns (total 10 columns):\n",
      "permalink        66368 non-null object\n",
      "name             66367 non-null object\n",
      "homepage_url     61310 non-null object\n",
      "category_list    63220 non-null object\n",
      "status           66368 non-null object\n",
      "country_code     59410 non-null object\n",
      "state_code       57821 non-null object\n",
      "region           58338 non-null object\n",
      "city             58340 non-null object\n",
      "founded_at       51147 non-null object\n",
      "dtypes: object(10)\n",
      "memory usage: 5.1+ MB\n",
      "None\n",
      "shape of dataset:  (66368, 10)\n",
      "variable dtypes:\n",
      " permalink        object\n",
      "name             object\n",
      "homepage_url     object\n",
      "category_list    object\n",
      "status           object\n",
      "country_code     object\n",
      "state_code       object\n",
      "region           object\n",
      "city             object\n",
      "founded_at       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#information of the companies dataset\n",
    "print(companies.info()); print(\"shape of dataset: \", companies.shape); print(\"variable dtypes:\\n\", companies.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114949 entries, 0 to 114948\n",
      "Data columns (total 6 columns):\n",
      "company_permalink          114949 non-null object\n",
      "funding_round_permalink    114949 non-null object\n",
      "funding_round_type         114949 non-null object\n",
      "funding_round_code         31140 non-null object\n",
      "funded_at                  114949 non-null object\n",
      "raised_amount_usd          94959 non-null float64\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 5.3+ MB\n",
      "None\n",
      "shape of dataset:  (114949, 6)\n",
      "variable dtypes:\n",
      " company_permalink           object\n",
      "funding_round_permalink     object\n",
      "funding_round_type          object\n",
      "funding_round_code          object\n",
      "funded_at                   object\n",
      "raised_amount_usd          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# information about the rounds dataset\n",
    "print(rounds.info()); print(\"shape of dataset: \", rounds.shape); print(\"variable dtypes:\\n\", rounds.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be focusing mostly on four variables only, let's remove all the extraneous variables from both the datasets. \n",
    "We'll remove from the companies dataset the following variables:\n",
    "    - state_code\n",
    "    - region\n",
    "    - city\n",
    "    - homepage_url\n",
    "    - founded_at\n",
    "    - name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary columns from companies\n",
    "companies.drop([\"state_code\", \"region\", \"city\", \"homepage_url\", \"founded_at\", \"name\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove the following from rounds:\n",
    "    - funded_at\n",
    "    - funding_round_code\n",
    "    - funding_round_permalink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary columns from rounds\n",
    "rounds.drop([\"funded_at\", \"funding_round_code\", \"funding_round_permalink\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q1: Number of unique companies in rounds\n",
    "To do this, we'll use the company_permalink column. However, instead of doing this directly, we'll first convert the company_permalink to lowercase and then determine the number of unique records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66370"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting company_permalink to lower case and getting number of unique records.\n",
    "rounds.company_permalink.str.lower().nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be 66370 unique companies in the dataset. This means that there are companies that had more than one round of funding. [Import Observation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checpoint 1 Q2: Number of unique companies in companies\n",
    "This time, we'll use the permalink, which is supposed to be the UID of a company. As with rounds.company_permalink, we'll first convert to lower case and then proceed to count the number of unique records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66368"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies.permalink.str.lower().nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a discrepancy between the number of unique records in companies and rounds. Does this mean that there are at least 2 companies in rounds that are not present in companies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q3: Key column to merge companies and rounds\n",
    "This is pretty easy. From the data dictionary we know the companies.permalink and rounds.company_permalink are UID's of each company in the dataset. So, we'll use companies.permalink as the key to merge with rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q4: Mismatches between rounds and companies\n",
    "Ok. Now, we're required to find out if there are any records that are unique to rounds only. That is these organizations are not present in companies but are present in rounds.\n",
    "\n",
    "We can do this by merging on companies.permalink and rounds.company_permalink. But, we'll take a slightly different approach here. \n",
    "\n",
    "First off, we'll create two new columns in rounds and companies called company_name and name resp. Then, we'll merge based on those columns and check for missing values. If there are missing values, then there are companies which are unique to rounds only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update\n",
    "So, I've found out that using a case-unified form of the permalinks produces the same result as using the names. Thus, to avoid creating unnecessary variables and excess storage consumption, I'll hold off on creating those extra columns and use the lower-case permalinks themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting rounds.company_permalink and companies.permalink to lower case\n",
    "companies[\"permalink\"] = companies.permalink.str.lower()\n",
    "\n",
    "rounds[\"company_permalink\"] = rounds.company_permalink.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29597                               /organization/e-cãbica\n",
      "31863          /organization/energystone-games-çµç³æ¸¸æ\n",
      "45176                  /organization/huizuche-com-æ ç§ÿè½¦\n",
      "58473                /organization/magnet-tech-ç£ç³ç§æ\n",
      "101036    /organization/tipcat-interactive-æ²èÿä¿¡æ¯ç...\n",
      "109969               /organization/weiche-tech-åè½¦ç§æ\n",
      "113839                   /organization/zengame-ç¦",
      "æ¸¸ç§æ\n",
      "Name: company_permalink, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# checking if there are any unique records.\n",
    "print(rounds.company_permalink[~rounds.company_permalink.isin(companies.permalink)].dropna());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Look at this stackoverflow answer: https://stackoverflow.com/questions/28901683/pandas-get-rows-which-are-not-in-other-dataframe for the full explanation of the code used above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be 7 companies that are in rounds but not in companies. So, the answer to the fourth question is yes. There are organizations that are present in rounds but not in companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q5: Merge the two dataframes\n",
    "This is the basis of all our analysis. Merging the two DataFrames will give us a single data frame which contains all the data needed. After this step, we can finally start treating the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after some thought it was decided unanimously that the best approach would be to drop all missing values from the raised_amount_usd column. Therefore, that's what I'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all missing values from the rounds DataFrame.\n",
    "rounds.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94959, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounds.shape # new shape of rounds: (94959, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping those missing values leaves us with 95K observations. So, we lost about 20K observations. Not a big problem though. The next step is to combine the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.merge(rounds, companies, how = \"inner\", left_on = \"company_permalink\", \n",
    "                  right_on = \"permalink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, performing this merge resulted in a DataFrame with 94958 rows and 7 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_code          0.061606\n",
       "category_list         0.010994\n",
       "status                0.000000\n",
       "permalink             0.000000\n",
       "raised_amount_usd     0.000000\n",
       "funding_round_type    0.000000\n",
       "company_permalink     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "master.isnull().mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two variables with missing values: country_code (an important variable) has 6% values missing and category_list has 1% values missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is to check for duplicates. If there are duplicates, just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1311"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for duplicates in the master DataFrame\n",
    "master.duplicated().sum() # there seem to be 1311 duplicated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.drop_duplicates(keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, dropping duplicates resulted in the loss of about 2K rows. We're now left with about 93K rows. This is about 81% of the initial observations. I don't think that deleting any more values is a good idea. This would result in the loss of more data, bringing the total down even more. \n",
    "\n",
    "Still, if we can retain more than 75% of the rows, we will have pretty reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treating the remaining missing values\n",
    "The remaining missing values are both present in Series of dtype object. This means that we can do something cool. We can treat the missing value itself as a new category. This is sweet. This means that we don't need to lose any more data. So, let's go ahead and do that. What we'll do is replace the NaN's with a new sentinel value that tells us if we know the country_code of an organization or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing missing values in country_code with new sentinel value\n",
    "master[\"country_code\"].fillna(\"unknown\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_list         0.011106\n",
       "country_code          0.000000\n",
       "status                0.000000\n",
       "permalink             0.000000\n",
       "raised_amount_usd     0.000000\n",
       "funding_round_type    0.000000\n",
       "company_permalink     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on running the following code, we see that the missing value percentage has gone down\n",
    "master.isnull().mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have about 1% of the total values missing 1% of it's data. 1% of 93647 is about 940. We can go ahead and delete these rows. We'll still be left with 80.5% of the total observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.dropna(inplace = True) # Now that we have this DataFrame, we can finally start with the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2: Funding Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 92607 entries, 0 to 94957\n",
      "Data columns (total 7 columns):\n",
      "company_permalink     92607 non-null object\n",
      "funding_round_type    92607 non-null object\n",
      "raised_amount_usd     92607 non-null float64\n",
      "permalink             92607 non-null object\n",
      "category_list         92607 non-null object\n",
      "status                92607 non-null object\n",
      "country_code          92607 non-null object\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 5.7+ MB\n",
      "None\n",
      "(92607, 7)\n"
     ]
    }
   ],
   "source": [
    "# checking the information about the master table after performing all the operations\n",
    "print(master.info()); print(master.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 3: Country Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 4: Sector Analysis 1\n",
    "So, let's start off with the sector analysis. Here's what needs to be done in this part of the analysis.\n",
    "    - Extract the primary sector of each category list from the category_list column.\n",
    "    - Use the mapping file 'mapping.csv' to map each primary sector to one of the eight main sectors (Note that ‘Others’ is also considered one of the main sectors)\n",
    "\n",
    "The primary sector is the string that appears before the first pipe \"|\" in the category_list variable. So, let's get on with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the primary sector from category_list\n",
    "First off, let's start by getting a good look at the dataframe. After that, we'll create a new variable called primary_sector to store the primary sector of each organization in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 92607 entries, 0 to 94957\n",
      "Data columns (total 7 columns):\n",
      "company_permalink     92607 non-null object\n",
      "funding_round_type    92607 non-null object\n",
      "raised_amount_usd     92607 non-null float64\n",
      "permalink             92607 non-null object\n",
      "category_list         92607 non-null object\n",
      "status                92607 non-null object\n",
      "country_code          92607 non-null object\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 5.7+ MB\n",
      "None\n",
      "(92607, 7)\n"
     ]
    }
   ],
   "source": [
    "print(master.info()); print(master.shape) # cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the primary sector\n",
    "master[\"primary_sector\"] = master.category_list.str.split(\"|\").str.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the primary sector, the next to-do item on our list is to map each of those primary sectors to a main sector. For that, we need the mapping data. We'll load that up and proceed to map the primary sectors to a main sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading mapping.csv as mapping\n",
    "mapping = pd.read_csv(\"../../Data/mapping.csv\", sep = \",\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 688 entries, 0 to 687\n",
      "Data columns (total 10 columns):\n",
      "category_list                              687 non-null object\n",
      "Automotive & Sports                        688 non-null int64\n",
      "Blanks                                     688 non-null int64\n",
      "Cleantech / Semiconductors                 688 non-null int64\n",
      "Entertainment                              688 non-null int64\n",
      "Health                                     688 non-null int64\n",
      "Manufacturing                              688 non-null int64\n",
      "News, Search and Messaging                 688 non-null int64\n",
      "Others                                     688 non-null int64\n",
      "Social, Finance, Analytics, Advertising    688 non-null int64\n",
      "dtypes: int64(9), object(1)\n",
      "memory usage: 53.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# some basic information about mapping\n",
    "print(mapping.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_list</th>\n",
       "      <th>Automotive &amp; Sports</th>\n",
       "      <th>Blanks</th>\n",
       "      <th>Cleantech / Semiconductors</th>\n",
       "      <th>Entertainment</th>\n",
       "      <th>Health</th>\n",
       "      <th>Manufacturing</th>\n",
       "      <th>News, Search and Messaging</th>\n",
       "      <th>Others</th>\n",
       "      <th>Social, Finance, Analytics, Advertising</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3D</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3D Printing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3D Technology</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_list  Automotive & Sports  Blanks  Cleantech / Semiconductors  \\\n",
       "0            NaN                    0       1                           0   \n",
       "1             3D                    0       0                           0   \n",
       "2    3D Printing                    0       0                           0   \n",
       "3  3D Technology                    0       0                           0   \n",
       "4     Accounting                    0       0                           0   \n",
       "\n",
       "   Entertainment  Health  Manufacturing  News, Search and Messaging  Others  \\\n",
       "0              0       0              0                           0       0   \n",
       "1              0       0              1                           0       0   \n",
       "2              0       0              1                           0       0   \n",
       "3              0       0              1                           0       0   \n",
       "4              0       0              0                           0       0   \n",
       "\n",
       "   Social, Finance, Analytics, Advertising  \n",
       "0                                        0  \n",
       "1                                        0  \n",
       "2                                        0  \n",
       "3                                        0  \n",
       "4                                        1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. So, this mapping file has turned out to be a one-to-one sparse matrix that maps each primary sector to one of the eight main sectors. What I have to do now is get a main sector for each primary sector. I'm not aware of any native pandas methods that enable me to do this. My first impulse is to write a function which will allow me to do just this. Then there's relational algebra which can produce results really quickly. I'll try out my function first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, this operation is what Hadley Wickham calls gathering the columns. (read more about it here: https://r4ds.had.co.nz/tidy-data.html).\n",
    "\n",
    "The main task here is this: convert the wide (and sparse) representation of the mapping into a long representation. This means bringing in all the colums under one roof. It's way easier to demonstrate than to explain. \n",
    "\n",
    "Luckily, pandas does provide a function to do this: pd.melt() (just like reshape2::melt() from R). Check the docs to understand how awesome this function is. (For a tutorial on pd.melt() go here: https://www.ibm.com/developerworks/community/blogs/jfp/entry/Tidy_Data_In_Python?lang=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the Blanks column since this just serves as a flag to identify NaN's\n",
    "mapping.drop(\"Blanks\", axis = 1, inplace = True)\n",
    "\n",
    "# dropping the single NaN at the head of the dataset\n",
    "mapping.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathering all the columns under one roof\n",
    "mapping_long = pd.melt(mapping, id_vars = [\"category_list\"], var_name = \"main_sector\", value_name = \"yes_no\")\n",
    "\n",
    "# tidying mapping_long to produce the final version of the mapping dataset\n",
    "mapping_tidy = mapping_long.loc[mapping_long.yes_no == 1, [\"category_list\", \"main_sector\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish up, let's take one look at the mapping_tidy dataset just to make sure everything is alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 687 entries, 7 to 5471\n",
      "Data columns (total 2 columns):\n",
      "category_list    687 non-null object\n",
      "main_sector      687 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 16.1+ KB\n",
      "None\n",
      "category_list    0.0\n",
      "main_sector      0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(mapping_tidy.info());\n",
    "print(mapping_tidy.isnull().mean()) # no null values. We can proceed to merge the two datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all that's left out is to merge the two datasets and proceed to perform a series of checks. Here are the final steps to finish up checkpoint 4.\n",
    "1. Merge master and mapping_tidy\n",
    "2. Check for: a. Missing values; b. Duplicates\n",
    "3. Treat missing values and drop duplicates`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue with merging\n",
    "Now, while merging the two datasets, master and mapped_tidy, one of two types of joins can be used. Either the inner or outer. There are the left and right joins, but we'll leave them out of the picture for now. \n",
    "\n",
    "If the inner join is used during the merge, there is a loss data. There are some sub-sectors that are present in master.primary_sector but aren't present in mapped.category_list. \n",
    "\n",
    "If the outer join is used during the merge, NULL values are inserted into the dataset. Again, dropping this is just equivalent to using the inner join. One other way to treat them is to manually add them to one of the 8 main sectors. \n",
    "\n",
    "What do you guys think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Analytics', 'Finance', 'Financial Services', 'Nanotechnology',\n",
       "       'Finance Technology', 'Business Analytics', 'Contact Management',\n",
       "       'Career Management', 'Big Data Analytics', 'Self Development',\n",
       "       'Cloud Management', 'Digital Signage', 'Cause Marketing',\n",
       "       'Personalization', 'Innovation Management', 'Risk Management',\n",
       "       'Document Management', 'Journalism', 'Energy Management',\n",
       "       'Waste Management', 'Fleet Management', 'Alternative Medicine',\n",
       "       'Real Estate Investors', 'Investment Management',\n",
       "       'English-Speaking', 'Intellectual Asset Management',\n",
       "       'Educational Games', 'Identity Management', 'Lead Management',\n",
       "       'Property Management', 'IT Management', 'Event Management',\n",
       "       'Navigation', 'Toys', 'Professional Services', 'Generation Y-Z',\n",
       "       'Cannabis', 'Enterprise Hardware', 'Social Media Advertising',\n",
       "       'Personal Finance', 'Darknet', 'Knowledge Management',\n",
       "       'China Internet', 'Medical Professionals',\n",
       "       'Digital Rights Management', 'Natural Gas Uses',\n",
       "       'Predictive Analytics', 'Natural Language Processing',\n",
       "       'Internet Technology', 'Nightlife', 'Adaptive Equipment',\n",
       "       'Project Management', 'Governance', 'Financial Exchanges',\n",
       "       'Professional Networking', 'Personal Branding',\n",
       "       'Supply Chain Management', 'Internet TV', 'Skill Gaming', 'Racing',\n",
       "       'Specialty Retail', 'Natural Resources', 'Swimming', 'Registrars',\n",
       "       'Personal Health', 'Golf Equipment',\n",
       "       'Biotechnology and Semiconductor', 'Vacation Rentals',\n",
       "       'Google Glass', 'Rapidly Expanding', 'Group Email',\n",
       "       'Mobile Analytics', 'Social Media Management', 'Kinect', 'Spas',\n",
       "       'Product Search', 'Sex Industry', 'Psychology', 'Promotional',\n",
       "       'Testing', 'GreenTech', 'Retirement', 'Subscription Businesses',\n",
       "       'Personal Data', 'Lingerie', 'Mobile Emergency&Health',\n",
       "       'Sponsorship', 'Deep Information Technology', 'Task Management'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primary sectors present in master.primary_sector but not present in mapping_tidy.category_list\n",
    "master.primary_sector[~master.primary_sector.isin(mapping_tidy.category_list)].dropna().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result of the code above, it's clear that there are 89 sub-sectors that are not included in the mapping dataset. So, if we're manually imputing the values, it would mean the addition of utmost, 89 lines of code. This would preserve the data, but decrease the readability of the file quite a bit. Since we're graded on code readability quite a bit, what do you think we should do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with this merging issue, here's what I've decided. By observation, I've found out that the following primary_sectors consitute about 90% of the missing values in the dataset. So, I've decided to place these primary_sectors into one of the 8 main_sectors. Thereby, significantly reducing the data loss that will occur if we drop the missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are primary sectors:\n",
    "Social, Analytics, Finance, Advertising:\n",
    "    - Analytics \n",
    "    - Finance \n",
    "    - Financial Services \n",
    "    - Finance Technology \n",
    "    - Business Analytics \n",
    "    - Big Data Analytics \n",
    "    - Investment Management \n",
    "    - Social Media Advertising \n",
    "    - Personal Finance \n",
    "    - Predictive Analytics \n",
    "    - Financial Exchanges\n",
    "    - Mobile Analytics \n",
    "    - Social Media Management\n",
    "    - Promotional\n",
    "Cleantech / Semiconductors:\n",
    "    - Waste Management\n",
    "    - Natural Gas Uses\n",
    "    - Biotechnology and Semiconductor\n",
    "    - Green Tech\n",
    "    - Energy Management\n",
    "    - Natural Resources\n",
    "Health:\n",
    "    - Alternative Medicine\n",
    "    - Cannabis\n",
    "    - Medical Professionals\n",
    "    - Personal Health\n",
    "    - Mobile Emergency&Health\n",
    "(I've segregated each of these primary sectors into one of the eight main sectors based on what I think is appropriate. Please go through the list and let me know if there are any changes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists of primary_sectors that fall under a main sector\n",
    "# Social, Finance, Analytics, Advertising\n",
    "social_analytics = [\"Analytics\", \"Finance\", \"Financial Services\", \"Finance Technology\", \"Business Analytics\", \n",
    "\"Big Data Analytics\", \"Investment Management\", \"Social Media Advertising\", \"Personal Finance\", \n",
    "\"Predictive Analytics\", \"Financial Exchanges\", \"Mobile Analytics\", \"Social Media Management\", \"Promotional\"]\n",
    "\n",
    "# Cleantech / Semiconductors\n",
    "cleantech_semiconductors = [\"Waste Management\", \"Natural Gas Uses\", \"Natural Resources\", \n",
    "\"Biotechnology and Semiconductor\", \"GreenTech\", \"Energy Management\"]\n",
    "\n",
    "# Health\n",
    "health = [\"Alternative Medicine\", \"Cannabis\", \"Meidcal Professionals\",\n",
    "\"Personal Health\", \"Mobile Emergency&Health\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the bins are ready, we'll perform the following steps in order:\n",
    "1. Merge the mapping_tidy dataset with the master dataset using an left join\n",
    "2. Impute the missing main_sector values in the merged master dataset\n",
    "3. Replace the other missing values in main_sector with the \"Others\" flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why I think deleting missing values is a bad idea at this stage?\n",
    "The answers for questions in Checkpoints 2 and 3 were found out with a dataset containing 92607 rows. Because of that, deleting any missing values, which might lead to the deletion of rows might bias the analysis and produce erroneous results. Therefore, I feel that it is better to flag the remaining 1000 rows as \"Others\" than to remove those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the master and mapping_tidy\n",
    "master = pd.merge(master, mapping_tidy, how = \"left\", left_on = \"primary_sector\", right_on = \"category_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the category_list_y variable and renaming category_list_x as category_list\n",
    "master.drop(\"category_list_y\", axis = 1, inplace = True)\n",
    "\n",
    "master.rename(index = str, columns = {\"category_list_x\": \"category_list\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling up the main_sector with appropriate values.\n",
    "# social, analytics, finance, advertising\n",
    "master.loc[master.primary_sector.isin(social_analytics), \"main_sector\"] = \"Social, Finance, Analytics, Advertising\"\n",
    "\n",
    "# cleantech / semiconductors\n",
    "master.loc[master.primary_sector.isin(cleantech_semiconductors), \"main_sector\"] = \"Cleantech / Semiconductors\"\n",
    "\n",
    "# health\n",
    "master.loc[master.primary_sector.isin(health), \"main_sector\"] = \"Health\"\n",
    "\n",
    "# filling up the remaining missing values with others\n",
    "master.loc[master.main_sector.isnull(), \"main_sector\"] = \"Others\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the dataset can be used to for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample code. Not for the final file. \n",
    "# converting raised_amount_usd to millions (divide 1000000)\n",
    "# master[\"raised_amount_usd\"] = master.raised_amount_usd / 1000000\n",
    "# master.groupby(\"main_sector\").raised_amount_usd.mean().sort_values(ascending = False)\n",
    "# master.groupby(\"funding_round_type\").raised_amount_usd.mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 5: Sector Analysis 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
