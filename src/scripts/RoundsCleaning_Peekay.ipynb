{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code for cleaning the rounds2 file of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Style\n",
    "    - Case: \n",
    "        - snake_case for objects\n",
    "        - camelCase for functions and classes\n",
    "    - Double quotes first, then single quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries used\n",
    "    - Pandas\n",
    "    - Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obejctives of Analysis\n",
    "Identify the most heavily invested main sectors in each of the three countries (for funding type FT and investments range of 5-15 M USD).\n",
    "\n",
    "Business objective: Identify the best: a. Sectors; b. Countries; c. Investment rounds for Spark Funds.\n",
    "\n",
    "[This means that we need to focus on just a few variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "Mean amount of money invested in a particular country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Workflow\n",
    "The workflow for this analysis is rather simple. Focus on answering the questions asked in the checkpoints. Following this flow, the code in this .ipynb is organized according to the checkpoints. There will be a clear heading indicating the starting and ending of each checkpoint and question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "# numpy\n",
    "import numpy as np # version: 1.15.0\n",
    "\n",
    "# pandas\n",
    "import pandas as pd # version: 0.23.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1: Data Cleaning\n",
    "There are five tasks in this checkpoint:\n",
    "    - Number of unique companies in rounds2.csv\n",
    "\t- Number of unique companies in companies.tsv\n",
    "\t- Key column from the companies dataset that can be used to merge it with rounds data\n",
    "\t- Organizations in companies that are missing in rounds2.\n",
    "    - Merge the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "The first step of the analysis is to import the two main datasets that we will be needing for the analysis: companies and rounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import companies.csv as companies\n",
    "companies = pd.read_csv(\"../../Data/companies.tsv\", sep = \"\\t\", encoding = \"ISO-8859-1\") \n",
    "\n",
    "# import rounds2.csv as rounds\n",
    "rounds = pd.read_csv(\"../../Data/rounds2.csv\", sep = \",\", encoding = \"ISO-8859-1\")\n",
    "# ISO for lack of charset in UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#information of the companies dataset\n",
    "print(companies.info()); print(\"shape of dataset: \", companies.shape); print(\"variable dtypes:\\n\", companies.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about the rounds dataset\n",
    "print(rounds.info()); print(\"shape of dataset: \", rounds.shape); print(\"variable dtypes:\\n\", rounds.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be focusing mostly on four variables only, let's remove all the extraneous variables from both the datasets. \n",
    "We'll remove from the companies dataset the following variables:\n",
    "    - state_code\n",
    "    - region\n",
    "    - city\n",
    "    - homepage_url\n",
    "    - founded_at\n",
    "    - name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary columns from companies\n",
    "companies.drop([\"state_code\", \"region\", \"city\", \"homepage_url\", \"founded_at\", \"name\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove the following from rounds:\n",
    "    - funded_at\n",
    "    - funding_round_code\n",
    "    - funding_round_permalink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecessary columns from rounds\n",
    "rounds.drop([\"funded_at\", \"funding_round_code\", \"funding_round_permalink\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q1: Number of unique companies in rounds\n",
    "To do this, we'll use the company_permalink column. However, instead of doing this directly, we'll first convert the company_permalink to lowercase and then determine the number of unique records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting company_permalink to lower case and getting number of unique records.\n",
    "rounds.company_permalink.str.lower().nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be 66370 unique companies in the dataset. This means that there are companies that had more than one round of funding. [Import Observation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checpoint 1 Q2: Number of unique companies in companies\n",
    "This time, we'll use the permalink, which is supposed to be the UID of a company. As with rounds.company_permalink, we'll first convert to lower case and then proceed to count the number of unique records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.permalink.str.lower().nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a discrepancy between the number of unique records in companies and rounds. Does this mean that there are at least 2 companies in rounds that are not present in companies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q3: Key column to merge companies and rounds\n",
    "This is pretty easy. From the data dictionary we know the companies.permalink and rounds.company_permalink are UID's of each company in the dataset. So, we'll use companies.permalink as the key to merge with rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q4: Mismatches between rounds and companies\n",
    "Ok. Now, we're required to find out if there are any records that are unique to rounds only. That is these organizations are not present in companies but are present in rounds.\n",
    "\n",
    "We can do this by merging on companies.permalink and rounds.company_permalink. But, we'll take a slightly different approach here. \n",
    "\n",
    "First off, we'll create two new columns in rounds and companies called company_name and name resp. Then, we'll merge based on those columns and check for missing values. If there are missing values, then there are companies which are unique to rounds only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update\n",
    "So, I've found out that using a case-unified form of the permalinks produces the same result as using the names. Thus, to avoid creating unnecessary variables and excess storage consumption, I'll hold off on creating those extra columns and use the lower-case permalinks themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting rounds.company_permalink and companies.permalink to lower case\n",
    "companies[\"permalink\"] = companies.permalink.str.lower()\n",
    "\n",
    "rounds[\"company_permalink\"] = rounds.company_permalink.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are any unique records.\n",
    "print(rounds.company_permalink[~rounds.company_permalink.isin(companies.permalink)].dropna());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Look at this stackoverflow answer: https://stackoverflow.com/questions/28901683/pandas-get-rows-which-are-not-in-other-dataframe for the full explanation of the code used above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be 7 companies that are in rounds but not in companies. So, the answer to the fourth question is yes. There are organizations that are present in rounds but not in companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1 Q5: Merge the two dataframes\n",
    "This is the basis of all our analysis. Merging the two DataFrames will give us a single data frame which contains all the data needed. After this step, we can finally start treating the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after some thought it was decided unanimously that the best approach would be to drop all missing values from the raised_amount_usd column. Therefore, that's what I'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all missing values from the rounds DataFrame.\n",
    "rounds.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds.shape # new shape of rounds: (94959, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping those missing values leaves us with 95K observations. So, we lost about 20K observations. Not a big problem though. The next step is to combine the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.merge(rounds, companies, how = \"inner\", left_on = \"company_permalink\", \n",
    "                  right_on = \"permalink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, performing this merge resulted in a DataFrame with 94958 rows and 7 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "master.isnull().mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two variables with missing values: country_code (an important variable) has 6% values missing and category_list has 1% values missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is to check for duplicates. If there are duplicates, just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates in the master DataFrame\n",
    "master.duplicated().sum() # there seem to be 1311 duplicated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.drop_duplicates(keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, dropping duplicates resulted in the loss of about 2K rows. We're now left with about 93K rows. This is about 81% of the initial observations. I don't think that deleting any more values is a good idea. This would result in the loss of more data, bringing the total down even more. \n",
    "\n",
    "Still, if we can retain more than 75% of the rows, we will have pretty reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treating the remaining missing values\n",
    "The remaining missing values are both present in Series of dtype object. This means that we can do something cool. We can treat the missing value itself as a new category. This is sweet. This means that we don't need to lose any more data. So, let's go ahead and do that. What we'll do is replace the NaN's with a new sentinel value that tells us if we know the country_code of an organization or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing missing values in country_code with new sentinel value\n",
    "master[\"country_code\"].fillna(\"unknown\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on running the following code, we see that the missing value percentage has gone down\n",
    "master.isnull().mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have about 1% of the total values missing 1% of it's data. 1% of 93647 is about 940. We can go ahead and delete these rows. We'll still be left with 80.5% of the total observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.dropna(inplace = True) # Now that we have this DataFrame, we can finally start with the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2: Funding Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the information about the master table after performing all the operations\n",
    "print(master.info()); print(master.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.1: Change the unit of 'raised_amount_usd' column\n",
    "\n",
    "Convert the unit of the `raised_amount_usd` from `$` to `million $`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for unit conversion here\n",
    "master[\"raised_amount_usd\"] = master[\"raised_amount_usd\"] / 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 2.2: Calculate the average investment amount for each of the four funding types (venture, angel, seed, and private equity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.groupby(\"funding_round_type\").raised_amount_usd.mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2.1: Average Values of Investments for Each of these Funding Types \n",
    " Average funding amount of venture type: 11.682595 (million USD) \t                              \n",
    " Average funding amount of angel type: 0.961039 (million USD)\t \n",
    " Average funding amount of seed type: 0.721313 (million USD)\t \n",
    " Average funding amount of private equity type: 73.350449 (million USD)\t \n",
    "\n",
    "Considering that Spark Funds wants to invest between 5 to 15 million USD per investment round, which investment type is the most suitable for it?\n",
    "\n",
    "venture\n",
    "\t                                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 3: Country Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  ### Subtask 3.1: Find the top nine countries with highest total funding for the given investment = \"venture\"\n",
    "\n",
    "    -- Spark Funds wants to see the top nine countries which have received the highest total funding (across ALL sectors for the chosen investment type)\n",
    "\n",
    "    -- For the chosen investment type, make a data frame named top9 with the top nine countries (based on the total investment amount each country has received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby command here to find the country-wise total funding for the investment type \n",
    "master_grpby_country= master.loc[master.funding_round_type == \"venture\", :].groupby(\"country_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to find the top nine countries with highest total funding\n",
    "top9 = master_grpby_country[\"raised_amount_usd\"].sum().sort_values(ascending=False)[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ### Subtask 3.2: Identify the top three English-speaking countries in the data frame top9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3.1: Analysing the Top 3 English-Speaking Countries\n",
    "Based on the list of countries where English is an official language - the top three English-speaking countries are:\n",
    "\n",
    " 1. Top English-speaking country:    USA (United States)             \n",
    " 2. Second English-speaking country: GBR (United Kingdom)\n",
    " 3. Third English-speaking country:  IND (India)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 4: Sector Analysis 1\n",
    "So, let's start off with the sector analysis. Here's what needs to be done in this part of the analysis.\n",
    "    - Extract the primary sector of each category list from the category_list column.\n",
    "    - Use the mapping file 'mapping.csv' to map each primary sector to one of the eight main sectors (Note that ‘Others’ is also considered one of the main sectors)\n",
    "\n",
    "The primary sector is the string that appears before the first pipe \"|\" in the category_list variable. So, let's get on with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the primary sector from category_list\n",
    "First off, let's start by getting a good look at the dataframe. After that, we'll create a new variable called primary_sector to store the primary sector of each organization in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(master.info()); print(master.shape) # cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the primary sector\n",
    "master[\"primary_sector\"] = master.category_list.str.split(\"|\").str.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the primary sector, the next to-do item on our list is to map each of those primary sectors to a main sector. For that, we need the mapping data. We'll load that up and proceed to map the primary sectors to a main sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading mapping.csv as mapping\n",
    "mapping = pd.read_csv(\"../../Data/mapping.csv\", sep = \",\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic information about mapping\n",
    "print(mapping.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. So, this mapping file has turned out to be a one-to-one sparse matrix that maps each primary sector to one of the eight main sectors. What I have to do now is get a main sector for each primary sector. I'm not aware of any native pandas methods that enable me to do this. My first impulse is to write a function which will allow me to do just this. Then there's relational algebra which can produce results really quickly. I'll try out my function first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, this operation is what Hadley Wickham calls gathering the columns. (read more about it here: https://r4ds.had.co.nz/tidy-data.html).\n",
    "\n",
    "The main task here is this: convert the wide (and sparse) representation of the mapping into a long representation. This means bringing in all the colums under one roof. It's way easier to demonstrate than to explain. \n",
    "\n",
    "Luckily, pandas does provide a function to do this: pd.melt() (just like reshape2::melt() from R). Check the docs to understand how awesome this function is. (For a tutorial on pd.melt() go here: https://www.ibm.com/developerworks/community/blogs/jfp/entry/Tidy_Data_In_Python?lang=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the Blanks column since this just serves as a flag to identify NaN's\n",
    "mapping.drop(\"Blanks\", axis = 1, inplace = True)\n",
    "\n",
    "# dropping the single NaN at the head of the dataset\n",
    "mapping.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathering all the columns under one roof\n",
    "mapping_long = pd.melt(mapping, id_vars = [\"category_list\"], var_name = \"main_sector\", value_name = \"yes_no\")\n",
    "\n",
    "# tidying mapping_long to produce the final version of the mapping dataset\n",
    "mapping_tidy = mapping_long.loc[mapping_long.yes_no == 1, [\"category_list\", \"main_sector\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish up, let's take one look at the mapping_tidy dataset just to make sure everything is alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping_tidy.info());\n",
    "print(mapping_tidy.isnull().mean()) # no null values. We can proceed to merge the two datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all that's left out is to merge the two datasets and proceed to perform a series of checks. Here are the final steps to finish up checkpoint 4.\n",
    "1. Merge master and mapping_tidy\n",
    "2. Check for: a. Missing values; b. Duplicates\n",
    "3. Treat missing values and drop duplicates`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue with merging\n",
    "Now, while merging the two datasets, master and mapped_tidy, one of two types of joins can be used. Either the inner or outer. There are the left and right joins, but we'll leave them out of the picture for now. \n",
    "\n",
    "If the inner join is used during the merge, there is a loss data. There are some sub-sectors that are present in master.primary_sector but aren't present in mapped.category_list. \n",
    "\n",
    "If the outer join is used during the merge, NULL values are inserted into the dataset. Again, dropping this is just equivalent to using the inner join. One other way to treat them is to manually add them to one of the 8 main sectors. \n",
    "\n",
    "What do you guys think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary sectors present in master.primary_sector but not present in mapping_tidy.category_list\n",
    "master.primary_sector[~master.primary_sector.isin(mapping_tidy.category_list)].dropna().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result of the code above, it's clear that there are 89 sub-sectors that are not included in the mapping dataset. So, if we're manually imputing the values, it would mean the addition of utmost, 89 lines of code. This would preserve the data, but decrease the readability of the file quite a bit. Since we're graded on code readability quite a bit, what do you think we should do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with this merging issue, here's what I've decided. By observation, I've found out that the following primary_sectors consitute about 90% of the missing values in the dataset. So, I've decided to place these primary_sectors into one of the 8 main_sectors. Thereby, significantly reducing the data loss that will occur if we drop the missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are primary sectors:\n",
    "Social, Analytics, Finance, Advertising:\n",
    "    - Analytics \n",
    "    - Finance \n",
    "    - Financial Services \n",
    "    - Finance Technology \n",
    "    - Business Analytics \n",
    "    - Big Data Analytics \n",
    "    - Investment Management \n",
    "    - Social Media Advertising \n",
    "    - Personal Finance \n",
    "    - Predictive Analytics \n",
    "    - Financial Exchanges\n",
    "    - Mobile Analytics \n",
    "    - Social Media Management\n",
    "    - Promotional\n",
    "Cleantech / Semiconductors:\n",
    "    - Waste Management\n",
    "    - Natural Gas Uses\n",
    "    - Biotechnology and Semiconductor\n",
    "    - Green Tech\n",
    "    - Energy Management\n",
    "    - Natural Resources\n",
    "Health:\n",
    "    - Alternative Medicine\n",
    "    - Cannabis\n",
    "    - Medical Professionals\n",
    "    - Personal Health\n",
    "    - Mobile Emergency&Health\n",
    "(I've segregated each of these primary sectors into one of the eight main sectors based on what I think is appropriate. Please go through the list and let me know if there are any changes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists of primary_sectors that fall under a main sector\n",
    "# Social, Finance, Analytics, Advertising\n",
    "social_analytics = [\"Analytics\", \"Finance\", \"Financial Services\", \"Finance Technology\", \"Business Analytics\", \n",
    "\"Big Data Analytics\", \"Investment Management\", \"Social Media Advertising\", \"Personal Finance\", \n",
    "\"Predictive Analytics\", \"Financial Exchanges\", \"Mobile Analytics\", \"Social Media Management\", \"Promotional\"]\n",
    "\n",
    "# Cleantech / Semiconductors\n",
    "cleantech_semiconductors = [\"Waste Management\", \"Natural Gas Uses\", \"Natural Resources\", \n",
    "\"Biotechnology and Semiconductor\", \"GreenTech\", \"Energy Management\"]\n",
    "\n",
    "# Health\n",
    "health = [\"Alternative Medicine\", \"Cannabis\", \"Meidcal Professionals\",\n",
    "\"Personal Health\", \"Mobile Emergency&Health\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the bins are ready, we'll perform the following steps in order:\n",
    "1. Merge the mapping_tidy dataset with the master dataset using an left join\n",
    "2. Impute the missing main_sector values in the merged master dataset\n",
    "3. Replace the other missing values in main_sector with the \"Others\" flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why I think deleting missing values is a bad idea at this stage?\n",
    "The answers for questions in Checkpoints 2 and 3 were found out with a dataset containing 92607 rows. Because of that, deleting any missing values, which might lead to the deletion of rows might bias the analysis and produce erroneous results. Therefore, I feel that it is better to flag the remaining 1000 rows as \"Others\" than to remove those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the master and mapping_tidy\n",
    "master = pd.merge(master, mapping_tidy, how = \"left\", left_on = \"primary_sector\", right_on = \"category_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the category_list_y variable and renaming category_list_x as category_list\n",
    "master.drop(\"category_list_y\", axis = 1, inplace = True)\n",
    "\n",
    "master.rename(index = str, columns = {\"category_list_x\": \"category_list\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling up the main_sector with appropriate values.\n",
    "# social, analytics, finance, advertising\n",
    "master.loc[master.primary_sector.isin(social_analytics), \"main_sector\"] = \"Social, Finance, Analytics, Advertising\"\n",
    "\n",
    "# cleantech / semiconductors\n",
    "master.loc[master.primary_sector.isin(cleantech_semiconductors), \"main_sector\"] = \"Cleantech / Semiconductors\"\n",
    "\n",
    "# health\n",
    "master.loc[master.primary_sector.isin(health), \"main_sector\"] = \"Health\"\n",
    "\n",
    "# filling up the remaining missing values with others\n",
    "master.loc[master.main_sector.isnull(), \"main_sector\"] = \"Others\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the dataset can be used to for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample code. Not for the final file. \n",
    "# converting raised_amount_usd to millions (divide 1000000)\n",
    "# master[\"raised_amount_usd\"] = master.raised_amount_usd / 1000000\n",
    "# master.groupby(\"main_sector\").raised_amount_usd.mean().sort_values(ascending = False)\n",
    "master.groupby(\"funding_round_type\").raised_amount_usd.mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 5: Sector Analysis 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to find out the most heavily invested main sectors in each of the three countries (for funding type FT and investments range of 5-15 M USD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the top 3 countries, and the preferred funding type, the last step is to identify the most preferred sectors in each country. To do this, we're required to create three dataframes, one for each country and get the total investment and counts for all of the main sectors. Here's how we'll tackle this:\n",
    "1. Create a dataframe for each country with the preferred funding type.\n",
    "2. Add the total investements and counts of investements in each sector to the dataframes.\n",
    "3. Fill out the table with the results we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the three dataframes\n",
    "# usa\n",
    "usa = master.loc[(master.country_code == \"USA\") & (master.funding_round_type == \"venture\"), :]\n",
    "\n",
    "# great britain\n",
    "gbr = master.loc[(master.country_code == \"GBR\") & (master.funding_round_type == \"venture\"), :]\n",
    "\n",
    "# india\n",
    "ind = master.loc[(master.country_code == \"IND\") & (master.funding_round_type == \"venture\"), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the total investements and adding them to the dataframes\n",
    "#usa\n",
    "usa_summary = usa.groupby(\"main_sector\").raised_amount_usd.agg([\"sum\", \"count\"])\n",
    "\n",
    "usa = pd.merge(usa, usa_summary, how = \"left\", on = \"main_sector\")\n",
    "\n",
    "# great britain\n",
    "gbr_summary = gbr.groupby(\"main_sector\").raised_amount_usd.agg([\"sum\", \"count\"])\n",
    "\n",
    "gbr = pd.merge(gbr, gbr_summary, how = \"left\", on = \"main_sector\")\n",
    "\n",
    "# india\n",
    "ind_summary = ind.groupby(\"main_sector\").raised_amount_usd.agg([\"sum\", \"count\"])\n",
    "\n",
    "ind = pd.merge(ind, ind_summary, how = \"left\", on = \"main_sector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataframes, it's time to answer the questions asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of investments in countries\n",
    "# usa\n",
    "usa.shape\n",
    "\n",
    "# great britain\n",
    "gbr.shape\n",
    "\n",
    "# india\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of investments in each country:\n",
    "USA: 35292\n",
    "GBR: 2027\n",
    "IND: 813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total size of investments in countries\n",
    "top3 = master.loc[(master.country_code.isin([\"USA\", \"GBR\", \"IND\"])) & (master.funding_round_type == \"venture\"), :]\n",
    "top3.groupby(\"country_code\").raised_amount_usd.agg([\"sum\", \"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total amount invested in each country:\n",
    "USA: 411102.768986\n",
    "GBR: 19931.867246\n",
    "IND: 14134.008718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top three sectors based on count of investments\n",
    "# usa\n",
    "usa.groupby(\"main_sector\").raised_amount_usd.count().sort_values(ascending = False)\n",
    "\n",
    "# great britain\n",
    "gbr.groupby(\"main_sector\").raised_amount_usd.count().sort_values(ascending = False)\n",
    "\n",
    "# india\n",
    "ind.groupby(\"main_sector\").raised_amount_usd.count().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Sector Based on Count of investment:\n",
    "USA: Others\n",
    "GBR: Others\n",
    "IND: Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second best sector based on count of investments:\n",
    "USA: Cleantech / Semiconductors\n",
    "GBR: Cleantech / Semiconductors\n",
    "IND: Social, Finance, Analytics, Advertising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third best secotr based on count of investments:\n",
    "USA: Social, Finance, Analytics, Advertising\n",
    "GBR: Social, Finance, Analytics, Advertising\n",
    "IND: News, Search and Messaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of investments in top sector\n",
    "USA:8521\n",
    "GBR:526\n",
    "IND:285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of investments in second best sector\n",
    "USA:7723\n",
    "GBR:436\n",
    "IND:144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of investments in third best sector\n",
    "USA:6984\n",
    "GBR:414\n",
    "IND:139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the top sector count-wise (point 3), which company received the highest investment?\n",
    "# usa\n",
    "usa[usa.main_sector == \"Others\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values()\n",
    "\n",
    "# gbr\n",
    "gbr[gbr.main_sector == \"Others\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values()\n",
    "\n",
    "# ind\n",
    "ind[ind.main_sector == \"Others\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the top sector count-wise (point 3), which company received the highest investment?\n",
    "USA: social-finance \n",
    "GBR: oneweb\n",
    "IND: flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the second-best sector which company received the highest investment?\n",
    "# usa\n",
    "usa[usa.main_sector == \"Cleantech / Semiconductors\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values()\n",
    "\n",
    "# gbr\n",
    "gbr[gbr.main_sector == \"Cleantech / Semiconductors\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values()\n",
    "\n",
    "# ind\n",
    "ind[ind.main_sector == \"Social, Finance, Analytics, Advertising\"].groupby(\"permalink\").raised_amount_usd.sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second-best sector count-wise (point 4), which company received the highest investment?\n",
    "USA: Freescale\n",
    "GBR: ImmunoCore\n",
    "IND: Shopclues.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
